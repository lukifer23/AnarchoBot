model:
  vocab_size: 32000
  n_layers: 12
  d_model: 768
  n_heads: 12
  mlp_multiple: 4.0
  dropout: 0.1
  max_seq_len: 4096
  rope_theta: 10000.0
  ffn_activation: silu
  norm_eps: 1e-5
  tie_embeddings: true

data:
  # Pre-tokenized SFT shards (memmap npy pairs)
  shard_dir: data/sft_ultrachat_shards
  format: npy
  dataset: json
  split: train
  text_field: messages
  seq_len: 2048
  shuffle_buffer: 0
  num_workers: 0
  cache_dir:
  streaming: false

train:
  total_steps: 10000        # ~20.5M tokens @ seq_len 2048, micro=1, accum=1
  micro_batch_size: 1
  grad_accum_steps: 1
  lr: 2.5e-4
  min_lr: 2.5e-5
  warmup_steps: 100
  weight_decay: 0.02
  max_grad_norm: 1.0
  log_interval: 10
  eval_interval: 500
  ckpt_interval: 500
  ckpt_keep: 3
  save_dir: checkpoints/sft_mlx
  precision: bfloat16
  compile: false
  gradient_checkpointing: true
  tokenizer_path: data/tokenizer.model
  optimizer: adamw
  checkpoint_path:
